{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156da348",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import gc\n",
    "import time\n",
    "import numpy as np\n",
    "import faiss\n",
    "import fitz  # PyMuPDF\n",
    "from tqdm import tqdm\n",
    "import ollama  # 确保已安装ollama Python包\n",
    "\n",
    "# 配置常量\n",
    "EMBEDDING_MODEL = \"nomic-embed-text\"  # 替换为你的嵌入模型\n",
    "VECTOR_STORE_DIR = \"./vector_stores\"  # 向量存储保存目录\n",
    "MODEL_NAME = \"llama3.2:latest\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0011c82c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 开始处理目录: /Users/stephenzhang/Desktop/AB_test\n",
      "发现 1 个PDF文件，开始处理...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "📄 处理PDF文件: 100%|██████████| 1/1 [00:00<00:00,  5.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 成功加载: Energy statistics- A class of statistics based on distances.pdf | 页数: 25 | 块数: 60\n",
      "\n",
      "🛠️ 开始创建向量存储 (共60个文本块)\n",
      "生成嵌入向量 (共60个块)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🧠 生成嵌入: 100%|██████████| 2/2 [00:05<00:00,  3.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "索引创建成功! 维度: 768 | 向量数: 60\n",
      "\n",
      "💾 保存向量存储到: ./vector_stores/my_documents.faiss\n",
      "✅ 向量存储构建完成! 索引大小: 60 个向量\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def build_pdf_vector_store(directory_path, index_name=\"default_index\", max_chars_per_page=50000, batch_size=50):\n",
    "    \"\"\"\n",
    "    端到端构建PDF向量存储系统\n",
    "    \"\"\"\n",
    "    # 创建存储目录\n",
    "    os.makedirs(VECTOR_STORE_DIR, exist_ok=True)\n",
    "    index_path = os.path.join(VECTOR_STORE_DIR, f\"{index_name}.faiss\")\n",
    "    \n",
    "    # 步骤1: 加载并处理PDF\n",
    "    print(f\"📂 开始处理目录: {directory_path}\")\n",
    "    all_chunks, metadata = load_pdfs_from_directory(directory_path, max_chars_per_page)\n",
    "    \n",
    "    if not all_chunks:\n",
    "        print(\"⚠️ 未找到可处理的PDF文件，终止操作\")\n",
    "        return None, None, None\n",
    "    \n",
    "    # 步骤2: 创建向量存储\n",
    "    print(f\"\\n🛠️ 开始创建向量存储 (共{len(all_chunks)}个文本块)\")\n",
    "    index, all_chunks, metadata = create_vector_store(all_chunks, metadata, batch_size)\n",
    "    \n",
    "    if index is None:\n",
    "        print(\"❌ 向量存储创建失败\")\n",
    "        return None, None, None\n",
    "    \n",
    "    # 步骤3: 保存向量存储\n",
    "    print(f\"\\n💾 保存向量存储到: {index_path}\")\n",
    "    faiss.write_index(index, index_path)\n",
    "    \n",
    "    # 保存元数据 (可选)\n",
    "    metadata_path = os.path.join(VECTOR_STORE_DIR, f\"{index_name}_metadata.pkl\")\n",
    "    with open(metadata_path, \"wb\") as f:\n",
    "        import pickle\n",
    "        pickle.dump({\"chunks\": all_chunks, \"metadata\": metadata}, f)\n",
    "    \n",
    "    print(f\"✅ 向量存储构建完成! 索引大小: {index.ntotal} 个向量\")\n",
    "    return index, all_chunks, metadata\n",
    "\n",
    "def load_pdfs_from_directory(directory_path, max_chars_per_page=50000):\n",
    "    \"\"\"流式加载PDF并提取文本，优化内存管理\"\"\"\n",
    "    all_chunks = []\n",
    "    metadata = []\n",
    "    \n",
    "    # 验证目录存在\n",
    "    if not os.path.exists(directory_path):\n",
    "        print(f\"❌ 目录不存在: {directory_path}\")\n",
    "        return [], []\n",
    "    \n",
    "    pdf_files = [f for f in os.listdir(directory_path) \n",
    "                 if f.lower().endswith('.pdf')]\n",
    "    \n",
    "    if not pdf_files:\n",
    "        print(f\"ℹ️ 目录中没有PDF文件: {directory_path}\")\n",
    "        return [], []\n",
    "    \n",
    "    print(f\"发现 {len(pdf_files)} 个PDF文件，开始处理...\")\n",
    "    \n",
    "    for filename in tqdm(pdf_files, desc=\"📄 处理PDF文件\"):\n",
    "        filepath = os.path.join(directory_path, filename)\n",
    "        try:\n",
    "            # 使用PyMuPDF打开（内存效率高）\n",
    "            with fitz.open(filepath) as doc:  # 使用上下文管理器自动关闭\n",
    "                total_chunks = 0\n",
    "                \n",
    "                for page_num in range(len(doc)):\n",
    "                    page = doc.load_page(page_num)\n",
    "                    \n",
    "                    # 核心文本提取（优化版）\n",
    "                    page_text = page.get_text(\"text\")  # 纯文本模式更快\n",
    "                    \n",
    "                    # 高效清理文本\n",
    "                    clean_text = re.sub(r'\\s+', ' ', page_text).strip()\n",
    "                    if len(clean_text) > max_chars_per_page:\n",
    "                        clean_text = clean_text[:max_chars_per_page]\n",
    "                        print(f\"⚠️ 截断超大页面: {filename} 第{page_num+1}页\")\n",
    "                    \n",
    "                    # 分块处理\n",
    "                    page_chunks = split_text(clean_text)\n",
    "                    total_chunks += len(page_chunks)\n",
    "                    \n",
    "                    # 收集数据\n",
    "                    for i, chunk in enumerate(page_chunks):\n",
    "                        all_chunks.append(chunk)\n",
    "                        metadata.append({\n",
    "                            'filename': filename,\n",
    "                            'filepath': filepath,\n",
    "                            'page': page_num + 1,\n",
    "                            'chunk_index': i,\n",
    "                            'total_chunks': len(page_chunks),\n",
    "                            'timestamp': time.time()\n",
    "                        })\n",
    "                    \n",
    "                    # 主动释放内存\n",
    "                    del page, page_text, clean_text\n",
    "                    page = None  # 确保解除引用\n",
    "                    \n",
    "                    # 每5页手动GC一次\n",
    "                    if page_num % 5 == 0:\n",
    "                        gc.collect()\n",
    "                \n",
    "                print(f\"✅ 成功加载: {filename} | 页数: {len(doc)} | 块数: {total_chunks}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"❌ 处理失败 {filename}: {str(e)[:200]}\")\n",
    "            # 失败时强制GC清理\n",
    "            gc.collect()\n",
    "    \n",
    "    return all_chunks, metadata\n",
    "\n",
    "def create_vector_store(all_chunks, metadata, batch_size=50):\n",
    "    \"\"\"批量创建向量存储，修复嵌入请求格式问题\"\"\"\n",
    "    if not all_chunks:\n",
    "        print(\"⚠️ 无文本块可用于创建向量存储\")\n",
    "        return None, [], []\n",
    "    \n",
    "    embeddings = []\n",
    "    failed_indices = []\n",
    "    print(f\"生成嵌入向量 (共{len(all_chunks)}个块)...\")\n",
    "    \n",
    "    # 分批处理避免内存溢出\n",
    "    for i in tqdm(range(0, len(all_chunks), batch_size), desc=\"🧠 生成嵌入\"):\n",
    "        batch = all_chunks[i:i+batch_size]\n",
    "        batch_embeddings = []  # 存储当前批次的嵌入\n",
    "        \n",
    "        try:\n",
    "            # 处理批处理中的每个文本块（逐个请求）\n",
    "            for j, text_chunk in enumerate(batch):\n",
    "                chunk_index = i + j  # 全局索引\n",
    "                \n",
    "                for attempt in range(3):  # 重试机制\n",
    "                    try:\n",
    "                        # 关键修复：prompt必须是单个字符串\n",
    "                        response = ollama.embeddings(\n",
    "                            model=EMBEDDING_MODEL,\n",
    "                            prompt=text_chunk  # 单个字符串而不是列表\n",
    "                        )\n",
    "                        \n",
    "                        # 确保我们获取到嵌入向量\n",
    "                        if 'embedding' in response:\n",
    "                            batch_embeddings.append(response['embedding'])\n",
    "                            break  # 成功则跳出重试循环\n",
    "                        else:\n",
    "                            raise ValueError(\"响应中缺少 'embedding' 字段\")\n",
    "                            \n",
    "                    except Exception as e:\n",
    "                        if attempt < 2:\n",
    "                            print(f\"🔄 重试 {attempt+1}/3: 块 {chunk_index} 嵌入请求失败 ({str(e)[:100]})\")\n",
    "                            time.sleep(2 ** attempt)  # 指数退避\n",
    "                        else:\n",
    "                            print(f\"❌ 块 {chunk_index} 嵌入请求最终失败: {str(e)[:200]}\")\n",
    "                            failed_indices.append(chunk_index)\n",
    "                            batch_embeddings.append(None)  # 占位符\n",
    "                            break\n",
    "            \n",
    "            # 添加到总嵌入列表\n",
    "            embeddings.extend(batch_embeddings)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ 处理批处理时发生意外错误: {str(e)[:200]}\")\n",
    "            # 标记整个批次为失败\n",
    "            failed_indices.extend(range(i, min(i+batch_size, len(all_chunks))))\n",
    "            embeddings.extend([None] * len(batch))\n",
    "    \n",
    "    # 移除失败的嵌入\n",
    "    if failed_indices:\n",
    "        print(f\"⚠️ 移除了 {len(failed_indices)} 个失败的嵌入\")\n",
    "        # 反向遍历避免索引错位\n",
    "        for idx in sorted(failed_indices, reverse=True):\n",
    "            del all_chunks[idx]\n",
    "            del metadata[idx]\n",
    "            if idx < len(embeddings):\n",
    "                del embeddings[idx]\n",
    "    \n",
    "    # 过滤掉None值\n",
    "    valid_embeddings = [e for e in embeddings if e is not None]\n",
    "    if not valid_embeddings:\n",
    "        print(\"❌ 无有效嵌入可用于创建索引\")\n",
    "        return None, [], []\n",
    "    \n",
    "    # 确保所有向量维度一致\n",
    "    dimension = len(valid_embeddings[0])\n",
    "    for emb in valid_embeddings:\n",
    "        if len(emb) != dimension:\n",
    "            print(f\"⚠️ 维度不一致: {len(emb)} vs {dimension}\")\n",
    "            # 处理不一致情况（这里简单跳过）\n",
    "            valid_embeddings.remove(emb)\n",
    "    \n",
    "    # 创建FAISS索引\n",
    "    try:\n",
    "        index = faiss.IndexFlatL2(dimension)\n",
    "        index.add(np.array(valid_embeddings, dtype=np.float32))\n",
    "        print(f\"索引创建成功! 维度: {dimension} | 向量数: {index.ntotal}\")\n",
    "        return index, all_chunks, metadata\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 创建FAISS索引失败: {str(e)[:200]}\")\n",
    "        return None, [], []\n",
    "\n",
    "def split_text(text, chunk_size=2000, overlap=200):\n",
    "    \"\"\"将文本分割为重叠的块，增强边界处理\"\"\"\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    \n",
    "    # 处理空文本\n",
    "    if not text.strip():\n",
    "        return []\n",
    "    \n",
    "    while start < len(text):\n",
    "        end = min(start + chunk_size, len(text))\n",
    "        chunk = text[start:end].strip()\n",
    "        \n",
    "        # 只在有内容时添加\n",
    "        if chunk:\n",
    "            chunks.append(chunk)\n",
    "        \n",
    "        # 检查是否到达文本末尾\n",
    "        if end == len(text):\n",
    "            break\n",
    "            \n",
    "        # 移动起始位置（考虑重叠）\n",
    "        start = end - overlap\n",
    "        if start < 0:\n",
    "            start = 0\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# 使用示例\n",
    "if __name__ == \"__main__\":\n",
    "    PDF_DIR = \"/Users/stephenzhang/Desktop/AB_test\"\n",
    "    INDEX_NAME = \"my_documents\"\n",
    "    \n",
    "    index, chunks, meta = build_pdf_vector_store(\n",
    "        directory_path=PDF_DIR,\n",
    "        index_name=INDEX_NAME,\n",
    "        max_chars_per_page=100000,  # 大页面限制\n",
    "        batch_size=30               # 小批量避免OOM\n",
    "    )\n",
    "    \n",
    "    if index:\n",
    "        # 示例：检索相似内容\n",
    "        query = \"什么是机器学习?\"\n",
    "        query_embed = ollama.embeddings(model=EMBEDDING_MODEL, prompt=query)['embedding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386ad1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_documents(question, index, chunks, metadata, model_name=MODEL_NAME, k=5):\n",
    "    \"\"\"\n",
    "    使用RAG（检索增强生成）技术回答问题\n",
    "    :param question: 用户问题\n",
    "    :param index: FAISS索引\n",
    "    :param chunks: 文本块列表\n",
    "    :param metadata: 元数据列表\n",
    "    :param model_name: 使用的Ollama模型名称\n",
    "    :param k: 返回的相似文本块数量\n",
    "    :return: 模型生成的答案\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 1. 将问题转换为嵌入向量\n",
    "        response = ollama.embeddings(\n",
    "            model=EMBEDDING_MODEL,\n",
    "            prompt=question\n",
    "        )\n",
    "        query_embedding = response['embedding']\n",
    "        \n",
    "        # 2. 在FAISS索引中搜索相似内容\n",
    "        query_embedding = np.array([query_embedding], dtype=np.float32)\n",
    "        distances, indices = index.search(query_embedding, k)\n",
    "        \n",
    "        # 3. 检索相关文本块\n",
    "        context_chunks = [chunks[i] for i in indices[0] if i < len(chunks)]\n",
    "        context_metadata = [metadata[i] for i in indices[0] if i < len(metadata)]\n",
    "        \n",
    "        # 4. 构造上下文提示\n",
    "        context = \"\\n\\n\".join([\n",
    "            f\"来源: {meta['filename']} 第{meta['page']}页\\n内容: {chunk}\"\n",
    "            for chunk, meta in zip(context_chunks, context_metadata)\n",
    "        ])\n",
    "        \n",
    "        # 5. 构造完整提示\n",
    "        prompt = f\"\"\"\n",
    "        你是一个专业的文档助手，请基于以下上下文信息回答问题。\n",
    "        如果信息不在上下文中，请如实回答你不知道。\n",
    "        \n",
    "        上下文:\n",
    "        {context}\n",
    "        \n",
    "        问题: {question}\n",
    "        回答:\n",
    "        \"\"\"\n",
    "        \n",
    "        # 6. 调用模型生成回答\n",
    "        response = ollama.chat(\n",
    "            model=model_name,\n",
    "            messages=[{'role': 'user', 'content': prompt}]\n",
    "        )\n",
    "        \n",
    "        return response['message']['content'], context_metadata\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"查询失败: {str(e)}\", []\n",
    "\n",
    "def interactive_chat(index, chunks, metadata, model_name=\"llama3\"):\n",
    "    \"\"\"与文档进行交互式对话\"\"\"\n",
    "    print(\"🚀 文档助手已启动! 输入 'exit' 退出\")\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            # 获取用户输入\n",
    "            question = input(\"\\n👤 您的问题: \")\n",
    "            if question.lower() in ['exit', 'quit']:\n",
    "                break\n",
    "                \n",
    "            if not question.strip():\n",
    "                continue\n",
    "                \n",
    "            # 查询文档\n",
    "            start_time = time.time()\n",
    "            answer, sources = query_documents(\n",
    "                question, index, chunks, metadata, model_name\n",
    "            )\n",
    "            elapsed = time.time() - start_time\n",
    "            \n",
    "            # 显示结果\n",
    "            print(f\"\\n🤖 助手 (响应时间: {elapsed:.2f}s):\")\n",
    "            print(answer)\n",
    "            \n",
    "            # 显示来源\n",
    "            if sources:\n",
    "                print(\"\\n📚 来源:\")\n",
    "                for i, meta in enumerate(sources):\n",
    "                    print(f\"{i+1}. {meta['filename']} - 第{meta['page']}页\")\n",
    "        \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\n再见!\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"❌ 错误: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8652f555",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paper-reader",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
