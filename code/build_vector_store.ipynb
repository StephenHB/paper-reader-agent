{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156da348",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import gc\n",
    "import time\n",
    "import numpy as np\n",
    "import faiss\n",
    "import fitz  # PyMuPDF\n",
    "from tqdm import tqdm\n",
    "import ollama  # ç¡®ä¿å·²å®‰è£…ollama PythonåŒ…\n",
    "\n",
    "# é…ç½®å¸¸é‡\n",
    "EMBEDDING_MODEL = \"nomic-embed-text\"  # æ›¿æ¢ä¸ºä½ çš„åµŒå…¥æ¨¡å‹\n",
    "VECTOR_STORE_DIR = \"./vector_stores\"  # å‘é‡å­˜å‚¨ä¿å­˜ç›®å½•\n",
    "MODEL_NAME = \"llama3.2:latest\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0011c82c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ å¼€å§‹å¤„ç†ç›®å½•: /Users/stephenzhang/Desktop/AB_test\n",
      "å‘ç° 1 ä¸ªPDFæ–‡ä»¶ï¼Œå¼€å§‹å¤„ç†...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ“„ å¤„ç†PDFæ–‡ä»¶: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  5.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… æˆåŠŸåŠ è½½: Energy statistics- A class of statistics based on distances.pdf | é¡µæ•°: 25 | å—æ•°: 60\n",
      "\n",
      "ğŸ› ï¸ å¼€å§‹åˆ›å»ºå‘é‡å­˜å‚¨ (å…±60ä¸ªæ–‡æœ¬å—)\n",
      "ç”ŸæˆåµŒå…¥å‘é‡ (å…±60ä¸ªå—)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ§  ç”ŸæˆåµŒå…¥: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:05<00:00,  3.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ç´¢å¼•åˆ›å»ºæˆåŠŸ! ç»´åº¦: 768 | å‘é‡æ•°: 60\n",
      "\n",
      "ğŸ’¾ ä¿å­˜å‘é‡å­˜å‚¨åˆ°: ./vector_stores/my_documents.faiss\n",
      "âœ… å‘é‡å­˜å‚¨æ„å»ºå®Œæˆ! ç´¢å¼•å¤§å°: 60 ä¸ªå‘é‡\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def build_pdf_vector_store(directory_path, index_name=\"default_index\", max_chars_per_page=50000, batch_size=50):\n",
    "    \"\"\"\n",
    "    ç«¯åˆ°ç«¯æ„å»ºPDFå‘é‡å­˜å‚¨ç³»ç»Ÿ\n",
    "    \"\"\"\n",
    "    # åˆ›å»ºå­˜å‚¨ç›®å½•\n",
    "    os.makedirs(VECTOR_STORE_DIR, exist_ok=True)\n",
    "    index_path = os.path.join(VECTOR_STORE_DIR, f\"{index_name}.faiss\")\n",
    "    \n",
    "    # æ­¥éª¤1: åŠ è½½å¹¶å¤„ç†PDF\n",
    "    print(f\"ğŸ“‚ å¼€å§‹å¤„ç†ç›®å½•: {directory_path}\")\n",
    "    all_chunks, metadata = load_pdfs_from_directory(directory_path, max_chars_per_page)\n",
    "    \n",
    "    if not all_chunks:\n",
    "        print(\"âš ï¸ æœªæ‰¾åˆ°å¯å¤„ç†çš„PDFæ–‡ä»¶ï¼Œç»ˆæ­¢æ“ä½œ\")\n",
    "        return None, None, None\n",
    "    \n",
    "    # æ­¥éª¤2: åˆ›å»ºå‘é‡å­˜å‚¨\n",
    "    print(f\"\\nğŸ› ï¸ å¼€å§‹åˆ›å»ºå‘é‡å­˜å‚¨ (å…±{len(all_chunks)}ä¸ªæ–‡æœ¬å—)\")\n",
    "    index, all_chunks, metadata = create_vector_store(all_chunks, metadata, batch_size)\n",
    "    \n",
    "    if index is None:\n",
    "        print(\"âŒ å‘é‡å­˜å‚¨åˆ›å»ºå¤±è´¥\")\n",
    "        return None, None, None\n",
    "    \n",
    "    # æ­¥éª¤3: ä¿å­˜å‘é‡å­˜å‚¨\n",
    "    print(f\"\\nğŸ’¾ ä¿å­˜å‘é‡å­˜å‚¨åˆ°: {index_path}\")\n",
    "    faiss.write_index(index, index_path)\n",
    "    \n",
    "    # ä¿å­˜å…ƒæ•°æ® (å¯é€‰)\n",
    "    metadata_path = os.path.join(VECTOR_STORE_DIR, f\"{index_name}_metadata.pkl\")\n",
    "    with open(metadata_path, \"wb\") as f:\n",
    "        import pickle\n",
    "        pickle.dump({\"chunks\": all_chunks, \"metadata\": metadata}, f)\n",
    "    \n",
    "    print(f\"âœ… å‘é‡å­˜å‚¨æ„å»ºå®Œæˆ! ç´¢å¼•å¤§å°: {index.ntotal} ä¸ªå‘é‡\")\n",
    "    return index, all_chunks, metadata\n",
    "\n",
    "def load_pdfs_from_directory(directory_path, max_chars_per_page=50000):\n",
    "    \"\"\"æµå¼åŠ è½½PDFå¹¶æå–æ–‡æœ¬ï¼Œä¼˜åŒ–å†…å­˜ç®¡ç†\"\"\"\n",
    "    all_chunks = []\n",
    "    metadata = []\n",
    "    \n",
    "    # éªŒè¯ç›®å½•å­˜åœ¨\n",
    "    if not os.path.exists(directory_path):\n",
    "        print(f\"âŒ ç›®å½•ä¸å­˜åœ¨: {directory_path}\")\n",
    "        return [], []\n",
    "    \n",
    "    pdf_files = [f for f in os.listdir(directory_path) \n",
    "                 if f.lower().endswith('.pdf')]\n",
    "    \n",
    "    if not pdf_files:\n",
    "        print(f\"â„¹ï¸ ç›®å½•ä¸­æ²¡æœ‰PDFæ–‡ä»¶: {directory_path}\")\n",
    "        return [], []\n",
    "    \n",
    "    print(f\"å‘ç° {len(pdf_files)} ä¸ªPDFæ–‡ä»¶ï¼Œå¼€å§‹å¤„ç†...\")\n",
    "    \n",
    "    for filename in tqdm(pdf_files, desc=\"ğŸ“„ å¤„ç†PDFæ–‡ä»¶\"):\n",
    "        filepath = os.path.join(directory_path, filename)\n",
    "        try:\n",
    "            # ä½¿ç”¨PyMuPDFæ‰“å¼€ï¼ˆå†…å­˜æ•ˆç‡é«˜ï¼‰\n",
    "            with fitz.open(filepath) as doc:  # ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨è‡ªåŠ¨å…³é—­\n",
    "                total_chunks = 0\n",
    "                \n",
    "                for page_num in range(len(doc)):\n",
    "                    page = doc.load_page(page_num)\n",
    "                    \n",
    "                    # æ ¸å¿ƒæ–‡æœ¬æå–ï¼ˆä¼˜åŒ–ç‰ˆï¼‰\n",
    "                    page_text = page.get_text(\"text\")  # çº¯æ–‡æœ¬æ¨¡å¼æ›´å¿«\n",
    "                    \n",
    "                    # é«˜æ•ˆæ¸…ç†æ–‡æœ¬\n",
    "                    clean_text = re.sub(r'\\s+', ' ', page_text).strip()\n",
    "                    if len(clean_text) > max_chars_per_page:\n",
    "                        clean_text = clean_text[:max_chars_per_page]\n",
    "                        print(f\"âš ï¸ æˆªæ–­è¶…å¤§é¡µé¢: {filename} ç¬¬{page_num+1}é¡µ\")\n",
    "                    \n",
    "                    # åˆ†å—å¤„ç†\n",
    "                    page_chunks = split_text(clean_text)\n",
    "                    total_chunks += len(page_chunks)\n",
    "                    \n",
    "                    # æ”¶é›†æ•°æ®\n",
    "                    for i, chunk in enumerate(page_chunks):\n",
    "                        all_chunks.append(chunk)\n",
    "                        metadata.append({\n",
    "                            'filename': filename,\n",
    "                            'filepath': filepath,\n",
    "                            'page': page_num + 1,\n",
    "                            'chunk_index': i,\n",
    "                            'total_chunks': len(page_chunks),\n",
    "                            'timestamp': time.time()\n",
    "                        })\n",
    "                    \n",
    "                    # ä¸»åŠ¨é‡Šæ”¾å†…å­˜\n",
    "                    del page, page_text, clean_text\n",
    "                    page = None  # ç¡®ä¿è§£é™¤å¼•ç”¨\n",
    "                    \n",
    "                    # æ¯5é¡µæ‰‹åŠ¨GCä¸€æ¬¡\n",
    "                    if page_num % 5 == 0:\n",
    "                        gc.collect()\n",
    "                \n",
    "                print(f\"âœ… æˆåŠŸåŠ è½½: {filename} | é¡µæ•°: {len(doc)} | å—æ•°: {total_chunks}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ å¤„ç†å¤±è´¥ {filename}: {str(e)[:200]}\")\n",
    "            # å¤±è´¥æ—¶å¼ºåˆ¶GCæ¸…ç†\n",
    "            gc.collect()\n",
    "    \n",
    "    return all_chunks, metadata\n",
    "\n",
    "def create_vector_store(all_chunks, metadata, batch_size=50):\n",
    "    \"\"\"æ‰¹é‡åˆ›å»ºå‘é‡å­˜å‚¨ï¼Œä¿®å¤åµŒå…¥è¯·æ±‚æ ¼å¼é—®é¢˜\"\"\"\n",
    "    if not all_chunks:\n",
    "        print(\"âš ï¸ æ— æ–‡æœ¬å—å¯ç”¨äºåˆ›å»ºå‘é‡å­˜å‚¨\")\n",
    "        return None, [], []\n",
    "    \n",
    "    embeddings = []\n",
    "    failed_indices = []\n",
    "    print(f\"ç”ŸæˆåµŒå…¥å‘é‡ (å…±{len(all_chunks)}ä¸ªå—)...\")\n",
    "    \n",
    "    # åˆ†æ‰¹å¤„ç†é¿å…å†…å­˜æº¢å‡º\n",
    "    for i in tqdm(range(0, len(all_chunks), batch_size), desc=\"ğŸ§  ç”ŸæˆåµŒå…¥\"):\n",
    "        batch = all_chunks[i:i+batch_size]\n",
    "        batch_embeddings = []  # å­˜å‚¨å½“å‰æ‰¹æ¬¡çš„åµŒå…¥\n",
    "        \n",
    "        try:\n",
    "            # å¤„ç†æ‰¹å¤„ç†ä¸­çš„æ¯ä¸ªæ–‡æœ¬å—ï¼ˆé€ä¸ªè¯·æ±‚ï¼‰\n",
    "            for j, text_chunk in enumerate(batch):\n",
    "                chunk_index = i + j  # å…¨å±€ç´¢å¼•\n",
    "                \n",
    "                for attempt in range(3):  # é‡è¯•æœºåˆ¶\n",
    "                    try:\n",
    "                        # å…³é”®ä¿®å¤ï¼špromptå¿…é¡»æ˜¯å•ä¸ªå­—ç¬¦ä¸²\n",
    "                        response = ollama.embeddings(\n",
    "                            model=EMBEDDING_MODEL,\n",
    "                            prompt=text_chunk  # å•ä¸ªå­—ç¬¦ä¸²è€Œä¸æ˜¯åˆ—è¡¨\n",
    "                        )\n",
    "                        \n",
    "                        # ç¡®ä¿æˆ‘ä»¬è·å–åˆ°åµŒå…¥å‘é‡\n",
    "                        if 'embedding' in response:\n",
    "                            batch_embeddings.append(response['embedding'])\n",
    "                            break  # æˆåŠŸåˆ™è·³å‡ºé‡è¯•å¾ªç¯\n",
    "                        else:\n",
    "                            raise ValueError(\"å“åº”ä¸­ç¼ºå°‘ 'embedding' å­—æ®µ\")\n",
    "                            \n",
    "                    except Exception as e:\n",
    "                        if attempt < 2:\n",
    "                            print(f\"ğŸ”„ é‡è¯• {attempt+1}/3: å— {chunk_index} åµŒå…¥è¯·æ±‚å¤±è´¥ ({str(e)[:100]})\")\n",
    "                            time.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿\n",
    "                        else:\n",
    "                            print(f\"âŒ å— {chunk_index} åµŒå…¥è¯·æ±‚æœ€ç»ˆå¤±è´¥: {str(e)[:200]}\")\n",
    "                            failed_indices.append(chunk_index)\n",
    "                            batch_embeddings.append(None)  # å ä½ç¬¦\n",
    "                            break\n",
    "            \n",
    "            # æ·»åŠ åˆ°æ€»åµŒå…¥åˆ—è¡¨\n",
    "            embeddings.extend(batch_embeddings)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ å¤„ç†æ‰¹å¤„ç†æ—¶å‘ç”Ÿæ„å¤–é”™è¯¯: {str(e)[:200]}\")\n",
    "            # æ ‡è®°æ•´ä¸ªæ‰¹æ¬¡ä¸ºå¤±è´¥\n",
    "            failed_indices.extend(range(i, min(i+batch_size, len(all_chunks))))\n",
    "            embeddings.extend([None] * len(batch))\n",
    "    \n",
    "    # ç§»é™¤å¤±è´¥çš„åµŒå…¥\n",
    "    if failed_indices:\n",
    "        print(f\"âš ï¸ ç§»é™¤äº† {len(failed_indices)} ä¸ªå¤±è´¥çš„åµŒå…¥\")\n",
    "        # åå‘éå†é¿å…ç´¢å¼•é”™ä½\n",
    "        for idx in sorted(failed_indices, reverse=True):\n",
    "            del all_chunks[idx]\n",
    "            del metadata[idx]\n",
    "            if idx < len(embeddings):\n",
    "                del embeddings[idx]\n",
    "    \n",
    "    # è¿‡æ»¤æ‰Noneå€¼\n",
    "    valid_embeddings = [e for e in embeddings if e is not None]\n",
    "    if not valid_embeddings:\n",
    "        print(\"âŒ æ— æœ‰æ•ˆåµŒå…¥å¯ç”¨äºåˆ›å»ºç´¢å¼•\")\n",
    "        return None, [], []\n",
    "    \n",
    "    # ç¡®ä¿æ‰€æœ‰å‘é‡ç»´åº¦ä¸€è‡´\n",
    "    dimension = len(valid_embeddings[0])\n",
    "    for emb in valid_embeddings:\n",
    "        if len(emb) != dimension:\n",
    "            print(f\"âš ï¸ ç»´åº¦ä¸ä¸€è‡´: {len(emb)} vs {dimension}\")\n",
    "            # å¤„ç†ä¸ä¸€è‡´æƒ…å†µï¼ˆè¿™é‡Œç®€å•è·³è¿‡ï¼‰\n",
    "            valid_embeddings.remove(emb)\n",
    "    \n",
    "    # åˆ›å»ºFAISSç´¢å¼•\n",
    "    try:\n",
    "        index = faiss.IndexFlatL2(dimension)\n",
    "        index.add(np.array(valid_embeddings, dtype=np.float32))\n",
    "        print(f\"ç´¢å¼•åˆ›å»ºæˆåŠŸ! ç»´åº¦: {dimension} | å‘é‡æ•°: {index.ntotal}\")\n",
    "        return index, all_chunks, metadata\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ åˆ›å»ºFAISSç´¢å¼•å¤±è´¥: {str(e)[:200]}\")\n",
    "        return None, [], []\n",
    "\n",
    "def split_text(text, chunk_size=2000, overlap=200):\n",
    "    \"\"\"å°†æ–‡æœ¬åˆ†å‰²ä¸ºé‡å çš„å—ï¼Œå¢å¼ºè¾¹ç•Œå¤„ç†\"\"\"\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    \n",
    "    # å¤„ç†ç©ºæ–‡æœ¬\n",
    "    if not text.strip():\n",
    "        return []\n",
    "    \n",
    "    while start < len(text):\n",
    "        end = min(start + chunk_size, len(text))\n",
    "        chunk = text[start:end].strip()\n",
    "        \n",
    "        # åªåœ¨æœ‰å†…å®¹æ—¶æ·»åŠ \n",
    "        if chunk:\n",
    "            chunks.append(chunk)\n",
    "        \n",
    "        # æ£€æŸ¥æ˜¯å¦åˆ°è¾¾æ–‡æœ¬æœ«å°¾\n",
    "        if end == len(text):\n",
    "            break\n",
    "            \n",
    "        # ç§»åŠ¨èµ·å§‹ä½ç½®ï¼ˆè€ƒè™‘é‡å ï¼‰\n",
    "        start = end - overlap\n",
    "        if start < 0:\n",
    "            start = 0\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# ä½¿ç”¨ç¤ºä¾‹\n",
    "if __name__ == \"__main__\":\n",
    "    PDF_DIR = \"/Users/stephenzhang/Desktop/AB_test\"\n",
    "    INDEX_NAME = \"my_documents\"\n",
    "    \n",
    "    index, chunks, meta = build_pdf_vector_store(\n",
    "        directory_path=PDF_DIR,\n",
    "        index_name=INDEX_NAME,\n",
    "        max_chars_per_page=100000,  # å¤§é¡µé¢é™åˆ¶\n",
    "        batch_size=30               # å°æ‰¹é‡é¿å…OOM\n",
    "    )\n",
    "    \n",
    "    if index:\n",
    "        # ç¤ºä¾‹ï¼šæ£€ç´¢ç›¸ä¼¼å†…å®¹\n",
    "        query = \"ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ?\"\n",
    "        query_embed = ollama.embeddings(model=EMBEDDING_MODEL, prompt=query)['embedding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386ad1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_documents(question, index, chunks, metadata, model_name=MODEL_NAME, k=5):\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰æŠ€æœ¯å›ç­”é—®é¢˜\n",
    "    :param question: ç”¨æˆ·é—®é¢˜\n",
    "    :param index: FAISSç´¢å¼•\n",
    "    :param chunks: æ–‡æœ¬å—åˆ—è¡¨\n",
    "    :param metadata: å…ƒæ•°æ®åˆ—è¡¨\n",
    "    :param model_name: ä½¿ç”¨çš„Ollamaæ¨¡å‹åç§°\n",
    "    :param k: è¿”å›çš„ç›¸ä¼¼æ–‡æœ¬å—æ•°é‡\n",
    "    :return: æ¨¡å‹ç”Ÿæˆçš„ç­”æ¡ˆ\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 1. å°†é—®é¢˜è½¬æ¢ä¸ºåµŒå…¥å‘é‡\n",
    "        response = ollama.embeddings(\n",
    "            model=EMBEDDING_MODEL,\n",
    "            prompt=question\n",
    "        )\n",
    "        query_embedding = response['embedding']\n",
    "        \n",
    "        # 2. åœ¨FAISSç´¢å¼•ä¸­æœç´¢ç›¸ä¼¼å†…å®¹\n",
    "        query_embedding = np.array([query_embedding], dtype=np.float32)\n",
    "        distances, indices = index.search(query_embedding, k)\n",
    "        \n",
    "        # 3. æ£€ç´¢ç›¸å…³æ–‡æœ¬å—\n",
    "        context_chunks = [chunks[i] for i in indices[0] if i < len(chunks)]\n",
    "        context_metadata = [metadata[i] for i in indices[0] if i < len(metadata)]\n",
    "        \n",
    "        # 4. æ„é€ ä¸Šä¸‹æ–‡æç¤º\n",
    "        context = \"\\n\\n\".join([\n",
    "            f\"æ¥æº: {meta['filename']} ç¬¬{meta['page']}é¡µ\\nå†…å®¹: {chunk}\"\n",
    "            for chunk, meta in zip(context_chunks, context_metadata)\n",
    "        ])\n",
    "        \n",
    "        # 5. æ„é€ å®Œæ•´æç¤º\n",
    "        prompt = f\"\"\"\n",
    "        ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£åŠ©æ‰‹ï¼Œè¯·åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”é—®é¢˜ã€‚\n",
    "        å¦‚æœä¿¡æ¯ä¸åœ¨ä¸Šä¸‹æ–‡ä¸­ï¼Œè¯·å¦‚å®å›ç­”ä½ ä¸çŸ¥é“ã€‚\n",
    "        \n",
    "        ä¸Šä¸‹æ–‡:\n",
    "        {context}\n",
    "        \n",
    "        é—®é¢˜: {question}\n",
    "        å›ç­”:\n",
    "        \"\"\"\n",
    "        \n",
    "        # 6. è°ƒç”¨æ¨¡å‹ç”Ÿæˆå›ç­”\n",
    "        response = ollama.chat(\n",
    "            model=model_name,\n",
    "            messages=[{'role': 'user', 'content': prompt}]\n",
    "        )\n",
    "        \n",
    "        return response['message']['content'], context_metadata\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"æŸ¥è¯¢å¤±è´¥: {str(e)}\", []\n",
    "\n",
    "def interactive_chat(index, chunks, metadata, model_name=\"llama3\"):\n",
    "    \"\"\"ä¸æ–‡æ¡£è¿›è¡Œäº¤äº’å¼å¯¹è¯\"\"\"\n",
    "    print(\"ğŸš€ æ–‡æ¡£åŠ©æ‰‹å·²å¯åŠ¨! è¾“å…¥ 'exit' é€€å‡º\")\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            # è·å–ç”¨æˆ·è¾“å…¥\n",
    "            question = input(\"\\nğŸ‘¤ æ‚¨çš„é—®é¢˜: \")\n",
    "            if question.lower() in ['exit', 'quit']:\n",
    "                break\n",
    "                \n",
    "            if not question.strip():\n",
    "                continue\n",
    "                \n",
    "            # æŸ¥è¯¢æ–‡æ¡£\n",
    "            start_time = time.time()\n",
    "            answer, sources = query_documents(\n",
    "                question, index, chunks, metadata, model_name\n",
    "            )\n",
    "            elapsed = time.time() - start_time\n",
    "            \n",
    "            # æ˜¾ç¤ºç»“æœ\n",
    "            print(f\"\\nğŸ¤– åŠ©æ‰‹ (å“åº”æ—¶é—´: {elapsed:.2f}s):\")\n",
    "            print(answer)\n",
    "            \n",
    "            # æ˜¾ç¤ºæ¥æº\n",
    "            if sources:\n",
    "                print(\"\\nğŸ“š æ¥æº:\")\n",
    "                for i, meta in enumerate(sources):\n",
    "                    print(f\"{i+1}. {meta['filename']} - ç¬¬{meta['page']}é¡µ\")\n",
    "        \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nå†è§!\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ é”™è¯¯: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8652f555",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paper-reader",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
